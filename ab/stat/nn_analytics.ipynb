{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "\n",
    "from ab.stat import export\n",
    "from ab.stat.util.Const import png_dir_raw, png_dir_stat, raw_xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if you need to import data from database and generate/update Excel file\n",
    "# export.main()\n",
    "\n",
    "# Create output directory for visualizations\n",
    "raw_img = png_dir_raw\n",
    "stat_img = png_dir_stat\n",
    "\n",
    "# Set plot style for publication-quality graphics\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"paper\", font_scale=1.3)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Read the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_excel(raw_xlsx)\n",
    "print(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Filtering and Preparation\n",
    "\n",
    "# Filter for relevant models and datasets\n",
    "relevant_models = [\n",
    "    'AirNet', 'AirNext', 'BagNet', 'DarkNet', 'Diffuser', 'DPN68', 'DPN131', 'DPN107', 'FractalNet', 'ICNet'\n",
    "]\n",
    "\n",
    "relevant_datasets = [\n",
    "    'celeba-gender', 'cifar-10', 'cifar-100', 'mnist', 'places365', 'svhn', 'imagenette'\n",
    "]\n",
    "\n",
    "filtered_df = df[(df['nn'].isin(relevant_models)) & (df['dataset'].isin(relevant_datasets))]\n",
    "filtered_df['duration_seconds'] = filtered_df['duration'] / 1e9\n",
    "print(f\"Filtered data shape: {filtered_df.shape}\")\n",
    "\n",
    "# Define color palette for consistent colors across visualizations\n",
    "model_colors = dict(zip(relevant_models, sns.color_palette(\"tab10\", len(relevant_models))))\n",
    "\n",
    "# Look at the filtered data\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 1: Overall Performance Comparison\n",
    "\n",
    "\n",
    "def create_performance_overview():\n",
    "    print(\"Creating performance overview visualization...\")\n",
    "    model_accuracy = filtered_df.groupby('nn')['accuracy'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = sns.barplot(x=model_accuracy.index, y=model_accuracy.values, palette=[model_colors[m] for m in model_accuracy.index])\n",
    "    \n",
    "    plt.title('Average Accuracy Across All Datasets', fontsize=16)\n",
    "    plt.xlabel('Neural Network Architecture', fontsize=14)\n",
    "    plt.ylabel('Average Accuracy', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage formatting to y-axis\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(model_accuracy.values):\n",
    "        ax.text(i, v + 0.01, f\"{v:.1%}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.ylim(0, max(model_accuracy.values) * 1.15)  # Add some space for labels\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'overall_performance.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_performance_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 2: Performance by Dataset - Heatmap\n",
    "\n",
    "def create_dataset_heatmap():\n",
    "    print(\"Creating dataset performance heatmap...\")\n",
    "    pivot_data = filtered_df.pivot_table(\n",
    "        index='dataset', \n",
    "        columns='nn', \n",
    "        values='accuracy', \n",
    "        aggfunc='max'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.heatmap(pivot_data, annot=True, fmt='.1%', cmap='YlGnBu', linewidths=0.5)\n",
    "    \n",
    "    plt.title('Maximum Accuracy by Model and Dataset', fontsize=16)\n",
    "    plt.xlabel('Neural Network Architecture', fontsize=14)\n",
    "    plt.ylabel('Dataset', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'dataset_heatmap.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_dataset_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 3: Training Efficiency\n",
    "\n",
    "def create_efficiency_visualization():\n",
    "    print(\"Creating training efficiency visualization...\")\n",
    "    # Calculate efficiency as accuracy per training second\n",
    "    efficiency_data = []\n",
    "    \n",
    "    for (nn, dataset), group in filtered_df.groupby(['nn', 'dataset']):\n",
    "        max_accuracy = group['accuracy'].max()\n",
    "        avg_duration = group['duration_seconds'].mean()\n",
    "        \n",
    "        if not pd.isna(avg_duration) and avg_duration > 0:\n",
    "            efficiency = max_accuracy / avg_duration\n",
    "            efficiency_data.append({\n",
    "                'nn': nn,\n",
    "                'dataset': dataset,\n",
    "                'efficiency': efficiency,\n",
    "                'max_accuracy': max_accuracy,\n",
    "                'avg_duration': avg_duration\n",
    "            })\n",
    "    \n",
    "    efficiency_df = pd.DataFrame(efficiency_data)\n",
    "    \n",
    "    # Create bubble chart - efficiency vs accuracy with size as duration\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for nn in relevant_models:\n",
    "        model_data = efficiency_df[efficiency_df['nn'] == nn]\n",
    "        if not model_data.empty:\n",
    "            sizes = model_data['avg_duration'] / model_data['avg_duration'].max() * 500\n",
    "            plt.scatter(\n",
    "                model_data['max_accuracy'], \n",
    "                model_data['efficiency'],\n",
    "                s=sizes,\n",
    "                alpha=0.7,\n",
    "                label=nn,\n",
    "                color=model_colors.get(nn)\n",
    "            )\n",
    "    \n",
    "    plt.title('Training Efficiency vs. Accuracy', fontsize=16)\n",
    "    plt.xlabel('Maximum Accuracy', fontsize=14)\n",
    "    plt.ylabel('Efficiency (Accuracy per Second)', fontsize=14)\n",
    "    plt.xscale('linear')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Neural Network')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'training_efficiency.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_efficiency_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 4: Learning Curves\n",
    "\n",
    "def create_learning_curves():\n",
    "    print(\"Creating learning curves for each dataset...\")\n",
    "    for dataset in relevant_datasets:\n",
    "        dataset_data = filtered_df[filtered_df['dataset'] == dataset]\n",
    "        \n",
    "        if dataset_data.empty:\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        for model in relevant_models:\n",
    "            model_data = dataset_data[dataset_data['nn'] == model]\n",
    "            \n",
    "            # Group by epoch and get mean accuracy\n",
    "            by_epoch = model_data.groupby('epoch')['accuracy'].mean().reset_index()\n",
    "            \n",
    "            if not by_epoch.empty:\n",
    "                plt.plot(\n",
    "                    by_epoch['epoch'], \n",
    "                    by_epoch['accuracy'], \n",
    "                    marker='o', \n",
    "                    markersize=5,\n",
    "                    linewidth=2, \n",
    "                    label=model,\n",
    "                    color=model_colors.get(model)\n",
    "                )\n",
    "        \n",
    "        plt.title(f'Learning Curves for {dataset}', fontsize=16)\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Accuracy', fontsize=14)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Neural Network')\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(raw_img / f'learning_curve_{dataset}.png')\n",
    "        \n",
    "        # Display the plot in the notebook\n",
    "        plt.show()\n",
    "\n",
    "# Run the function to create and display learning curves for each dataset\n",
    "create_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 5: Convergence Speed\n",
    "\n",
    "def create_convergence_visualization():\n",
    "    print(\"Creating convergence speed visualization...\")\n",
    "    # Calculate epochs to convergence for each model-dataset combination\n",
    "    convergence_data = []\n",
    "    \n",
    "    for (nn, dataset), group in filtered_df.groupby(['nn', 'dataset']):\n",
    "        # Group by epoch and get mean accuracy\n",
    "        by_epoch = group.groupby('epoch')['accuracy'].mean().reset_index()\n",
    "        \n",
    "        if not by_epoch.empty:\n",
    "            max_accuracy = by_epoch['accuracy'].max()\n",
    "            convergence_epoch = by_epoch.loc[by_epoch['accuracy'].idxmax(), 'epoch']\n",
    "            \n",
    "            convergence_data.append({\n",
    "                'nn': nn,\n",
    "                'dataset': dataset,\n",
    "                'max_accuracy': max_accuracy,\n",
    "                'convergence_epoch': convergence_epoch\n",
    "            })\n",
    "    \n",
    "    convergence_df = pd.DataFrame(convergence_data)\n",
    "    \n",
    "    # Find fastest converging model for each dataset that achieves at least 90% of best accuracy\n",
    "    fastest_models = []\n",
    "    \n",
    "    for dataset, dataset_group in convergence_df.groupby('dataset'):\n",
    "        best_accuracy = dataset_group['max_accuracy'].max()\n",
    "        threshold = best_accuracy * 0.9\n",
    "        \n",
    "        valid_models = dataset_group[dataset_group['max_accuracy'] >= threshold]\n",
    "        if not valid_models.empty:\n",
    "            fastest = valid_models.loc[valid_models['convergence_epoch'].idxmin()]\n",
    "            fastest_models.append(fastest)\n",
    "    \n",
    "    fastest_df = pd.DataFrame(fastest_models)\n",
    "    \n",
    "    # Create bar chart of convergence epochs\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = sns.barplot(\n",
    "        x='dataset',\n",
    "        y='convergence_epoch',\n",
    "        hue='nn',\n",
    "        data=fastest_df,\n",
    "        palette=[model_colors.get(m) for m in fastest_df['nn']]\n",
    "    )\n",
    "    \n",
    "    plt.title('Fastest Converging Model by Dataset', fontsize=16)\n",
    "    plt.xlabel('Dataset', fontsize=14)\n",
    "    plt.ylabel('Epochs to Convergence', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add accuracy annotations\n",
    "    for i, row in enumerate(fastest_df.itertuples()):\n",
    "        ax.text(\n",
    "            i, \n",
    "            row.convergence_epoch + 0.5, \n",
    "            f\"{row.max_accuracy:.1%}\", \n",
    "            ha='center', \n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    plt.legend(title='Neural Network')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'convergence_speed.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_convergence_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 6: Dataset Difficulty\n",
    "\n",
    "def create_dataset_difficulty_visualization():\n",
    "    print(\"Creating dataset difficulty visualization...\")\n",
    "    # Calculate average accuracy across models for each dataset\n",
    "    dataset_difficulty = filtered_df.groupby('dataset')['accuracy'].mean().sort_values()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(\n",
    "        x=dataset_difficulty.index, \n",
    "        y=dataset_difficulty.values,\n",
    "        palette='Blues_d'\n",
    "    )\n",
    "    \n",
    "    plt.title('Average Accuracy by Dataset (Difficulty)', fontsize=16)\n",
    "    plt.xlabel('Dataset', fontsize=14)\n",
    "    plt.ylabel('Average Accuracy', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage formatting to y-axis\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(dataset_difficulty.values):\n",
    "        ax.text(i, v + 0.01, f\"{v:.1%}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'dataset_difficulty.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_dataset_difficulty_visualization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization 7: Best Model for Each Dataset\n",
    "\n",
    "def create_best_model_visualization():\n",
    "    print(\"Creating best model visualization...\")\n",
    "    best_models = filtered_df.loc[filtered_df.groupby('dataset')['accuracy'].idxmax()]\n",
    "    \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = sns.barplot(\n",
    "        x='dataset', \n",
    "        y='accuracy', \n",
    "        hue='nn', \n",
    "        data=best_models,\n",
    "        palette=[model_colors.get(m) for m in best_models['nn']]\n",
    "    )\n",
    "    \n",
    "    plt.title('Best Performing Model for Each Dataset', fontsize=16)\n",
    "    plt.xlabel('Dataset', fontsize=14)\n",
    "    plt.ylabel('Maximum Accuracy', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage formatting to y-axis\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, row in enumerate(best_models.itertuples()):\n",
    "        ax.text(i, row.accuracy + 0.01, f\"{row.accuracy:.1%}\", ha='center', fontsize=10)\n",
    "    \n",
    "    plt.legend(title='Neural Network')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'best_models.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_best_model_visualization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Visualization 8: All Models Performance for Each Dataset\n",
    "\n",
    "def create_all_models_performance_visualization():\n",
    "    print(\"Creating all models performance visualization...\")\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.boxplot(\n",
    "        x='dataset', \n",
    "        y='accuracy', \n",
    "        hue='nn', \n",
    "        data=filtered_df,\n",
    "        palette=[model_colors.get(m) for m in relevant_models]\n",
    "    )\n",
    "    \n",
    "    plt.title('Performance of All Models for Each Dataset', fontsize=16)\n",
    "    plt.xlabel('Dataset', fontsize=14)\n",
    "    plt.ylabel('Accuracy', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add percentage formatting to y-axis\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0))\n",
    "    \n",
    "    plt.legend(title='Neural Network', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(stat_img / 'all_models_performance.png')\n",
    "    \n",
    "    # Display the plot in the notebook\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create and display the visualization\n",
    "create_all_models_performance_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
